{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/guywaffo/neural-bood-pressure-estimation?scriptVersionId=103988589\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"4889a61f","metadata":{"papermill":{"duration":0.004281,"end_time":"2022-08-23T07:53:50.815706","exception":false,"start_time":"2022-08-23T07:53:50.811425","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]},"source":["## Network"]},{"cell_type":"code","execution_count":1,"id":"bde33938","metadata":{"execution":{"iopub.execute_input":"2022-08-23T07:53:50.824592Z","iopub.status.busy":"2022-08-23T07:53:50.823479Z","iopub.status.idle":"2022-08-23T07:53:50.834728Z","shell.execute_reply":"2022-08-23T07:53:50.83392Z"},"papermill":{"duration":0.017967,"end_time":"2022-08-23T07:53:50.836838","exception":false,"start_time":"2022-08-23T07:53:50.818871","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n"]},{"cell_type":"code","execution_count":2,"id":"76b0524a","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-08-23T07:53:50.844113Z","iopub.status.busy":"2022-08-23T07:53:50.843816Z","iopub.status.idle":"2022-08-23T07:53:56.653749Z","shell.execute_reply":"2022-08-23T07:53:56.65271Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":5.816716,"end_time":"2022-08-23T07:53:56.656509","exception":false,"start_time":"2022-08-23T07:53:50.839793","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"outputs":[],"source":["# Inspiration from AlexNet CNNF\n","'''The network consists of 5 Convolutional (CONV) layers and 3 Fully Connected (FC) layers.\n","The activation used is the Rectified Linear Unit (ReLU).\n","params:\n","data_in-shape : Tensor containing the ppg time series. Size (batch_size, ppg_length, 1) batch_size=128\n","fs : sampling frequency (default fs = 125 Hz)\n","'''\n","#Import libraries\n","import tensorflow as tf\n","from keras.layers import Reshape\n","from tensorflow.keras.layers import Softmax,Bidirectional,LSTM, Permute, Input, Add, Conv1D, MaxPooling1D, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling1D, MaxPooling2D, GlobalMaxPooling2D, LeakyReLU, GlobalAveragePooling2D, ReLU, Dropout\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.keras.models import Model\n","\n","\n","def AlexNet_1D(data_in_shape, num_output=2, dil=1, kernel_size=3, fs = 125, useMaxPooling=True, UseDerivative=False):\n","\n","    # Define the input as a tensor with shape input_shape\n","    X_input = Input(shape=data_in_shape)\n","\n","    if UseDerivative:\n","        dt1 = (X_input[:,1:] - X_input[:,:-1])*fs\n","        dt2 = (dt1[:,1:] - dt1[:,:-1])*fs\n","\n","        dt1 = tf.pad(dt1, tf.constant([[0,0],[0,1],[0,0]]))\n","        dt2 = tf.pad(dt2, tf.constant([[0,0],[0,2],[0,0]]))\n","        X = tf.concat([X_input, dt1, dt2], axis=2)\n","    else:        # X=tf.keras.layers.Concatenate(axis=2)([X_input, dt1,dt2])\n","        X=X_input\n","\n","    # convolutional stage\n","    X = Conv1D(filters=2, kernel_size=350, strides=1, name='conv1', kernel_initializer=glorot_uniform(seed=0), padding=\"same\")(X)\n","    X = Activation(ReLU())(X)\n","    X = BatchNormalization(axis=-1, name='BatchNorm1')(X)\n","    X = MaxPooling1D(175, strides=1, name=\"MaxPool1\",padding=\"same\")(X)\n","    \n","\n","    X = Conv1D(filters=10, kernel_size=175, strides=1, name='conv2', kernel_initializer=glorot_uniform(seed=0), padding=\"same\")(X)\n","    X = Activation(ReLU())(X)\n","    X = BatchNormalization(axis=-1, name='BatchNorm2')(X)\n","    X = MaxPooling1D(25, strides=1, name=\"MaxPool2\",padding=\"same\")(X)\n","\n","    X = Conv1D(filters=20, kernel_size=25, strides=1, name='conv3', kernel_initializer=glorot_uniform(seed=0), padding=\"same\")(X)\n","    X = Activation(ReLU())(X)\n","    X = BatchNormalization(axis=-1, name='BatchNorm3')(X)\n","    X = MaxPooling1D(10, strides=1, name=\"Maxpool3\",padding=\"same\")(X)\n","\n","    X = Conv1D(filters=40, kernel_size=10, strides=1, name='conv4', kernel_initializer=glorot_uniform(seed=0), padding=\"same\")(X)\n","    X = Activation(ReLU())(X)\n","    X = BatchNormalization(axis=-1, name='BatchNorm4')(X)\n","    X = MaxPooling1D(5, strides=1, name=\"Maxpool4\",padding=\"same\")(X)\n","\n","    X = Conv1D(filters=40, kernel_size=5, strides=1, name='conv5', kernel_initializer=glorot_uniform(seed=0),\n","               padding=\"same\")(X)\n","    X = Activation(ReLU())(X)\n","    X = BatchNormalization(axis=-1, name='BatchNorm5')(X)\n","    X = MaxPooling1D(2, strides=1, name=\"Maxpool5\", padding=\"same\")(X)\n","\n","    #Flattening the output of the NN\n","    X= Flatten()(X)\n","    X= Reshape((875,40))(X)\n","    #Bi-LSTM stage\n","    X = Bidirectional(LSTM(256, return_sequences=True),merge_mode='concat')(X)\n","    X = Bidirectional(LSTM(512, return_sequences=True),merge_mode='concat')(X)\n","\n","    # Fully connected slayer\n","    X = Flatten()(X)\n","    X = Dense( 2, activation='relu', name='dense', kernel_initializer=glorot_uniform(seed=0))(X)\n","    X = Dropout(rate=0.5)(X)\n","\n","\n","    # output stage\n","    X_SBP = Dense(1, activation='relu', name='SBP', kernel_initializer=glorot_uniform(seed=0))(X)\n","    X_DBP = Dense(1, activation='relu', name='DBP', kernel_initializer=glorot_uniform(seed=0))(X)\n","    model = Model(inputs=X_input, outputs=[X_SBP, X_DBP], name='AlexNet_1D')\n","    return model\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a61a3c55","metadata":{"papermill":{"duration":0.002851,"end_time":"2022-08-23T07:53:56.662564","exception":false,"start_time":"2022-08-23T07:53:56.659713","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"4ae6574c","metadata":{"papermill":{"duration":0.002761,"end_time":"2022-08-23T07:53:56.66862","exception":false,"start_time":"2022-08-23T07:53:56.665859","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]},"source":["## Training\n"]},{"cell_type":"code","execution_count":3,"id":"597fd30c","metadata":{"execution":{"iopub.execute_input":"2022-08-23T07:53:56.675938Z","iopub.status.busy":"2022-08-23T07:53:56.675322Z","iopub.status.idle":"2022-08-23T07:53:56.680056Z","shell.execute_reply":"2022-08-23T07:53:56.679125Z"},"papermill":{"duration":0.010618,"end_time":"2022-08-23T07:53:56.682108","exception":false,"start_time":"2022-08-23T07:53:56.67149","status":"completed"},"tags":[]},"outputs":[],"source":["\n","CHECKPOINTS_FILE=\"/kaggle/input/outputstrainingphase2/logs/checkpoints/2022-22-08_alexnet_training_kaggle_cb.h5\""]},{"cell_type":"code","execution_count":4,"id":"498d1c91","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-08-23T07:53:56.689422Z","iopub.status.busy":"2022-08-23T07:53:56.689151Z","iopub.status.idle":"2022-08-23T07:53:56.951951Z","shell.execute_reply":"2022-08-23T07:53:56.948873Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.269242,"end_time":"2022-08-23T07:53:56.954199","exception":false,"start_time":"2022-08-23T07:53:56.684957","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-08-23 07:53:56.799084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-23 07:53:56.939588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-23 07:53:56.940375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"]}],"source":["\"\"\" train neural architectures using PPG data\n","\n","This script trains a neural network using PPG data. The data is loaded from the the .tfrecord files created by the script\n","'hdf_to_tfrecord.py'.\n","\"\"\"\n","import csv\n","import os.path\n","import warnings\n","from os.path import expanduser, join\n","from os import environ\n","from sys import argv\n","from functools import partial\n","from datetime import datetime\n","import argparse\n","\n","import tensorflow as tf\n","# tf.compat.v1.disable_eager_execution()\n","\n","\n","import pandas as pd\n","import numpy as np\n","import glob\n","\n","from tqdm import tqdm\n","\n","\n","gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n","for device in gpu_devices:\n","    tf.config.experimental.set_memory_growth(device, True)\n","\n","BATCH_SIZE=54\n","WIN_LEN=875\n","def read_tfrecord(example, win_len=WIN_LEN):\n","    tfrecord_format = (\n","        {\n","            'ppg': tf.io.FixedLenFeature([win_len], tf.float32),\n","            'label': tf.io.FixedLenFeature([2], tf.float32)\n","        }\n","    )\n","    parsed_features = tf.io.parse_single_example(example, tfrecord_format)\n","\n","    return parsed_features['ppg'], (parsed_features['label'][0], parsed_features['label'][1])\n","\n","\n","def create_dataset(tfrecords_dir, tfrecord_basename, win_len=WIN_LEN, batch_size=BATCH_SIZE, modus='train'):\n","    # pattern = join(tfrecords_dir, modus, tfrecord_basename + \"_\" + modus + \"_?????_of_?????.tfrecord\")\n","    pattern = glob.glob(tfrecords_dir + f\"/{modus}/*.tfrecord\")\n","    dataset = tf.data.TFRecordDataset.from_tensor_slices(pattern)\n","\n","    if modus == 'train':\n","        dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n","        dataset = dataset.interleave(\n","            tf.data.TFRecordDataset,\n","            cycle_length=800,\n","            block_length=400)\n","    else:\n","        dataset = dataset.interleave(\n","            tf.data.TFRecordDataset)\n","\n","    dataset = dataset.map(partial(read_tfrecord, win_len=win_len), num_parallel_calls=2)\n","    dataset = dataset.shuffle(4096, reshuffle_each_iteration=True)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch_size, drop_remainder=False)\n","    dataset = dataset.repeat()\n","    \n","    return dataset\n","\n","\n","def get_model(architecture, input_shape, UseDerivative=False):\n","    return {\n","        'alexnet': AlexNet_1D(input_shape, UseDerivative=UseDerivative),\n","    }[architecture]\n","\n","\n","def ppg_train_mimic_iii(architecture,\n","                        DataDir,\n","                        ResultsDir,\n","                        CheckpointDir,\n","                        tensorboard_tag,\n","                        tfrecord_basename,\n","                        experiment_name,\n","                        win_len=WIN_LEN,\n","                        batch_size=BATCH_SIZE,\n","                        lr=None,\n","                        N_epochs=20,\n","                        Ntrain=1e6,\n","                        Nval=2.5e5,\n","                        Ntest=2.5e5,\n","                        UseDerivative=False,\n","                        earlystopping=False):\n","    # create datasets for training, validation and testing using .tfrecord files\n","    test_dataset = create_dataset(DataDir, tfrecord_basename, win_len=win_len, batch_size=batch_size,\n","                                  modus='test')\n","    train_dataset = create_dataset(DataDir, tfrecord_basename, win_len=win_len, batch_size=batch_size, modus='train')\n","    val_dataset = create_dataset(DataDir, tfrecord_basename, win_len=win_len, batch_size=batch_size,\n","                                 modus='val')\n","    data_in_shape = (win_len, 1)\n","\n","    # load the neurarchitecture\n","    model = get_model(architecture, data_in_shape, UseDerivative=UseDerivative)\n","\n","    # callback for logging training and validation results\n","    csvLogger_cb = tf.keras.callbacks.CSVLogger(\n","        filename=join(ResultsDir, experiment_name + '_learningcurve.csv')\n","    )\n","\n","    # checkpoint callback\n","    cb_file=join(CheckpointDir, experiment_name + '_cb.h5')\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=cb_file,\n","        save_best_only=True\n","    )\n","    \n","    if os.path.exists(CHECKPOINTS_FILE):\n","        model.load_weights(CHECKPOINTS_FILE)\n","        print(\"Loaded weights from checkpoint\",CHECKPOINTS_FILE)\n","\n","    # tensorboard callback\n","    tensorbard_cb = tf.keras.callbacks.TensorBoard(\n","        log_dir=join(ResultsDir, 'tb', tensorboard_tag),\n","        histogram_freq=0,\n","        update_freq=\"batch\",\n","        write_images=True,\n","        write_graph=True    )\n","\n","    # callback for early stopping if validation loss stops improving\n","    EarlyStopping_cb = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        patience=10,\n","        restore_best_weights=True\n","    )\n","\n","\n","    # define Adam optimizer\n","    if lr is None:\n","        opt = tf.keras.optimizers.Adam()\n","    else:\n","        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","    # compile model using mean squared error as loss function\n","    model.compile(\n","        optimizer=opt,\n","        loss=tf.keras.losses.mean_squared_error,\n","        metrics=[['mae'], ['mae']]\n","    )\n","\n","\n","\n","    cb_list = [checkpoint_cb,\n","               tensorbard_cb,\n","               csvLogger_cb,\n","               EarlyStopping_cb if earlystopping == True else [],\n","               tf.keras.callbacks.ReduceLROnPlateau(monitor='mae', factor=0.2, patience=5)]\n","\n","\n","\n","\n","\n","    # # Perform Training and Validation\n","    history = model.fit(\n","        train_dataset,\n","        steps_per_epoch=Ntrain // batch_size,\n","        epochs=N_EPOCHS,\n","        validation_data=val_dataset,\n","        validation_steps=Nval // batch_size,\n","        callbacks=cb_list\n","    )\n","    # Predictions on the testset\n","    print(\"Loading weights from \", checkpoint_cb.filepath)\n","    model.load_weights(checkpoint_cb.filepath)\n","    test_results = pd.DataFrame({'SBP_true': [],\n","                                 'DBP_true': [],\n","                                 'SBP_est': [],\n","                                 'DBP_est': []})\n","\n","    # store predictions on the test set as well as the corresponding ground truth in a csv file\n","    test_dataset = iter(test_dataset)\n","    for i in tqdm(range(int(Ntest // batch_size)), \"Running test\"):\n","        ppg_test, BP_true = test_dataset.next()\n","        BP_est = model.predict(ppg_test, verbose=0)\n","        with warnings.catch_warnings():\n","            TestBatchResult = pd.DataFrame({'SBP_true': BP_true[0].numpy(),\n","                                            'DBP_true': BP_true[1].numpy(),\n","                                            'SBP_est': np.squeeze(BP_est[0]),\n","                                            'DBP_est': np.squeeze(BP_est[1]),\n","                                            })\n","            test_results = test_results.append(TestBatchResult)\n","\n","    ResultsFile = join(ResultsDir, experiment_name + '_test_results.csv')\n","    test_results.to_csv(ResultsFile)\n","\n","    ResultsFileMae = join(ResultsDir, experiment_name + '_test_results_ae.csv')\n","\n","    sbp_mae = np.mean(np.abs(test_results[\"SBP_true\"] - test_results[\"SBP_est\"]))\n","    sbp_aestd = np.std(np.abs(test_results[\"SBP_true\"] - test_results[\"SBP_est\"]))\n","\n","    dbp_mae = np.mean(np.abs(test_results[\"DBP_true\"] - test_results[\"DBP_est\"]))\n","    dbp_aestd = np.std(np.abs(test_results[\"DBP_true\"] - test_results[\"DBP_est\"]))\n","\n","    with open(ResultsFileMae, \"w\") as output:\n","        writer = csv.writer(output)\n","        writer.writerow([\"sbp_mae\", \"sbp_aestd\", \"dbp_mae\", \"dbp_aestd\"])\n","        writer.writerow([sbp_mae, sbp_aestd, dbp_mae, dbp_aestd])\n","    test_results.to_csv(ResultsFile)\n","\n","    idx_min = np.argmin(history.history['val_loss'])\n","\n","    print(' Training finished')\n","\n","    return history.history['SBP_mae'][idx_min], history.history['DBP_mae'][idx_min], history.history['val_SBP_mae'][\n","        idx_min], history.history['val_DBP_mae'][idx_min]\n"]},{"cell_type":"markdown","id":"9bcc3152","metadata":{"papermill":{"duration":0.002903,"end_time":"2022-08-23T07:53:56.960672","exception":false,"start_time":"2022-08-23T07:53:56.957769","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]},"source":["# Launch training"]},{"cell_type":"code","execution_count":5,"id":"aa80ef58","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-08-23T07:53:56.96779Z","iopub.status.busy":"2022-08-23T07:53:56.967494Z","iopub.status.idle":"2022-08-23T17:29:33.747485Z","shell.execute_reply":"2022-08-23T17:29:33.746499Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":34536.786104,"end_time":"2022-08-23T17:29:33.749704","exception":false,"start_time":"2022-08-23T07:53:56.9636","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-08-23 07:53:57.080988: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-08-23 07:53:57.081443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-23 07:53:57.082310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-23 07:53:57.082970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-23 07:53:59.379812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-23 07:53:59.380716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-23 07:53:59.381393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-08-23 07:53:59.382078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"]},{"name":"stdout","output_type":"stream","text":["Loaded weights from checkpoint /kaggle/input/outputstrainingphase2/logs/checkpoints/2022-22-08_alexnet_training_kaggle_cb.h5\n"]},{"name":"stderr","output_type":"stream","text":["2022-08-23 07:54:01.648045: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n","2022-08-23 07:54:01.648094: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n","2022-08-23 07:54:01.650084: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n","2022-08-23 07:54:01.864616: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n","2022-08-23 07:54:01.864795: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["2022-08-23 07:54:07.177242: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","2022-08-23 07:54:09.773451: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"]},{"name":"stdout","output_type":"stream","text":["   1/7812 [..............................] - ETA: 33:13:18 - loss: 1125.9451 - SBP_loss: 997.6653 - DBP_loss: 128.2798 - SBP_mae: 24.8021 - DBP_mae: 8.8565"]},{"name":"stderr","output_type":"stream","text":["2022-08-23 07:54:17.590378: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n","2022-08-23 07:54:17.590440: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"]},{"name":"stdout","output_type":"stream","text":["   2/7812 [..............................] - ETA: 3:15:53 - loss: 1134.7095 - SBP_loss: 989.6103 - DBP_loss: 145.0992 - SBP_mae: 25.1477 - DBP_mae: 9.6215 "]},{"name":"stderr","output_type":"stream","text":["2022-08-23 07:54:18.874872: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n","2022-08-23 07:54:18.891587: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n","2022-08-23 07:54:19.037845: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 18385 callback api events and 18379 activity events. \n","2022-08-23 07:54:19.260000: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n","2022-08-23 07:54:19.666206: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19\n","\n","2022-08-23 07:54:19.929715: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19/6885ba6d848e.trace.json.gz\n","2022-08-23 07:54:20.174369: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19\n","\n","2022-08-23 07:54:20.184562: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19/6885ba6d848e.memory_profile.json.gz\n","2022-08-23 07:54:20.190050: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19\n","Dumped tool data for xplane.pb to /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19/6885ba6d848e.xplane.pb\n","Dumped tool data for overview_page.pb to /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19/6885ba6d848e.overview_page.pb\n","Dumped tool data for input_pipeline.pb to /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19/6885ba6d848e.input_pipeline.pb\n","Dumped tool data for tensorflow_stats.pb to /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19/6885ba6d848e.tensorflow_stats.pb\n","Dumped tool data for kernel_stats.pb to /kaggle/working/logs/results/tb/2022-23-08_alexnet_training_kaggle/train/plugins/profile/2022_08_23_07_54_19/6885ba6d848e.kernel_stats.pb\n","\n"]},{"name":"stdout","output_type":"stream","text":["7812/7812 [==============================] - 6671s 852ms/step - loss: 1058.1508 - SBP_loss: 902.6947 - DBP_loss: 155.4537 - SBP_mae: 23.4764 - DBP_mae: 9.8488 - val_loss: 784.0736 - val_SBP_loss: 621.8586 - val_DBP_loss: 162.2172 - val_SBP_mae: 19.6005 - val_DBP_mae: 10.0636\n","Epoch 2/5\n","7812/7812 [==============================] - 6659s 852ms/step - loss: 887.5916 - SBP_loss: 732.5728 - DBP_loss: 155.0186 - SBP_mae: 21.1004 - DBP_mae: 9.8309 - val_loss: 724.0665 - val_SBP_loss: 562.3187 - val_DBP_loss: 161.7477 - val_SBP_mae: 18.7389 - val_DBP_mae: 10.0295\n","Epoch 3/5\n","7812/7812 [==============================] - 6657s 852ms/step - loss: 777.5097 - SBP_loss: 623.2415 - DBP_loss: 154.2688 - SBP_mae: 19.6485 - DBP_mae: 9.8032 - val_loss: 696.8899 - val_SBP_loss: 538.3494 - val_DBP_loss: 158.5404 - val_SBP_mae: 18.4977 - val_DBP_mae: 9.9029\n","Epoch 4/5\n","7812/7812 [==============================] - 6629s 849ms/step - loss: 726.1310 - SBP_loss: 572.8419 - DBP_loss: 153.2913 - SBP_mae: 19.1350 - DBP_mae: 9.7632 - val_loss: 690.5853 - val_SBP_loss: 532.0200 - val_DBP_loss: 158.5652 - val_SBP_mae: 18.5890 - val_DBP_mae: 9.8950\n","Epoch 5/5\n","7812/7812 [==============================] - 6628s 848ms/step - loss: 716.3915 - SBP_loss: 563.6575 - DBP_loss: 152.7345 - SBP_mae: 19.1561 - DBP_mae: 9.7420 - val_loss: 687.7458 - val_SBP_loss: 529.3850 - val_DBP_loss: 158.3602 - val_SBP_mae: 18.6047 - val_DBP_mae: 9.9029\n","Loading weights from  /kaggle/working/logs/checkpoints/2022-23-08_alexnet_training_kaggle_cb.h5\n"]},{"name":"stderr","output_type":"stream","text":["Running test: 100%|██████████| 1953/1953 [20:57<00:00,  1.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":[" Training finished\n"]},{"data":{"text/plain":["(19.156070709228516, 9.742035865783691, 18.60467529296875, 9.902948379516602)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["architecture = \"alexnet\"\n","experiment_name = \"training_kaggle\"\n","experiment_name = datetime.now().strftime(\"%Y-%d-%m\") + '_' + architecture + '_' + experiment_name\n","# experiment_name=\"2022-08-08_alexnet_training4\"\n","ROOT_DIR=\"/kaggle/working/logs\"\n","DataDir = \"/kaggle/input/mmicdataset/train_test\"\n","ResultsDir = os.path.join(ROOT_DIR, \"results\")\n","CheckpointDir = os.path.join(ROOT_DIR,\"checkpoints\")\n","tb_tag = experiment_name\n","lr = 0.001\n","batch_size = 128\n","WIN_LEN = 875\n","N_EPOCHS = 5\n","\n","\n","tfrecord_basename = 'MIMIC_III_ppg'\n","\n","ppg_train_mimic_iii(architecture,\n","                    DataDir,\n","                    ResultsDir,\n","                    CheckpointDir,\n","                    tb_tag,\n","                    tfrecord_basename,\n","                    experiment_name,\n","                    win_len=WIN_LEN,\n","                    batch_size=batch_size,\n","                    lr=lr,\n","                    N_epochs=N_EPOCHS,\n","                    UseDerivative=False,\n","                    earlystopping=False)  # False originally\n"]},{"cell_type":"code","execution_count":null,"id":"4d100955","metadata":{"papermill":{"duration":2.377647,"end_time":"2022-08-23T17:29:38.350694","exception":false,"start_time":"2022-08-23T17:29:35.973047","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"555c4fec","metadata":{"papermill":{"duration":2.585056,"end_time":"2022-08-23T17:29:43.169279","exception":false,"start_time":"2022-08-23T17:29:40.584223","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":34565.989583,"end_time":"2022-08-23T17:29:48.808141","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-08-23T07:53:42.818558","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/guywaffo/neural-bood-pressure-estimation?scriptVersionId=103647210\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Network","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import os\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspiration from AlexNet CNNF\n'''The network consists of 5 Convolutional (CONV) layers and 3 Fully Connected (FC) layers.\nThe activation used is the Rectified Linear Unit (ReLU).\nparams:\ndata_in-shape : Tensor containing the ppg time series. Size (batch_size, ppg_length, 1) batch_size=128\nfs : sampling frequency (default fs = 125 Hz)\n'''\n#Import libraries\nimport tensorflow as tf\nfrom keras.layers import Reshape\nfrom tensorflow.keras.layers import Softmax,Bidirectional,LSTM, Permute, Input, Add, Conv1D, MaxPooling1D, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling1D, MaxPooling2D, GlobalMaxPooling2D, LeakyReLU, GlobalAveragePooling2D, ReLU, Dropout\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.models import Model\n\n\ndef AlexNet_1D(data_in_shape, num_output=2, dil=1, kernel_size=3, fs = 125, useMaxPooling=True, UseDerivative=False):\n\n    # Define the input as a tensor with shape input_shape\n    X_input = Input(shape=data_in_shape)\n\n    if UseDerivative:\n        dt1 = (X_input[:,1:] - X_input[:,:-1])*fs\n        dt2 = (dt1[:,1:] - dt1[:,:-1])*fs\n\n        dt1 = tf.pad(dt1, tf.constant([[0,0],[0,1],[0,0]]))\n        dt2 = tf.pad(dt2, tf.constant([[0,0],[0,2],[0,0]]))\n        X = tf.concat([X_input, dt1, dt2], axis=2)\n    else:        # X=tf.keras.layers.Concatenate(axis=2)([X_input, dt1,dt2])\n        X=X_input\n\n    # convolutional stage\n    X = Conv1D(filters=2, kernel_size=350, strides=1, name='conv1', kernel_initializer=glorot_uniform(seed=0), padding=\"same\")(X)\n    X = Activation(ReLU())(X)\n    X = BatchNormalization(axis=-1, name='BatchNorm1')(X)\n    X = MaxPooling1D(175, strides=1, name=\"MaxPool1\",padding=\"same\")(X)\n\n\n    X = Conv1D(filters=10, kernel_size=175, strides=1, name='conv2', kernel_initializer=glorot_uniform(seed=0), padding=\"same\")(X)\n    X = Activation(ReLU())(X)\n    X = BatchNormalization(axis=-1, name='BatchNorm2')(X)\n    X = MaxPooling1D(25, strides=1, name=\"MaxPool2\",padding=\"same\")(X)\n\n    X = Conv1D(filters=20, kernel_size=25, strides=1, name='conv3', kernel_initializer=glorot_uniform(seed=0), padding=\"same\")(X)\n    X = Activation(ReLU())(X)\n    X = BatchNormalization(axis=-1, name='BatchNorm3')(X)\n    X = MaxPooling1D(10, strides=1, name=\"Maxpool3\",padding=\"same\")(X)\n\n    X = Conv1D(filters=40, kernel_size=10, strides=1, name='conv4', kernel_initializer=glorot_uniform(seed=0), padding=\"same\")(X)\n    X = Activation(ReLU())(X)\n    X = BatchNormalization(axis=-1, name='BatchNorm4')(X)\n    X = MaxPooling1D(4, strides=1, name=\"Maxpool4\",padding=\"same\")(X)\n\n    #Flattening the output of the CNN\n    X= Flatten()(X)\n    X= Reshape((875,40))(X)\n    #Bi-LSTM stage\n    X = Bidirectional(LSTM(128, return_sequences=True),merge_mode='concat')(X)\n    X = Bidirectional(LSTM(350, return_sequences=True),merge_mode='concat')(X)\n\n    # Fully connected slayer\n    X = Flatten()(X)\n    X = Dense( 2, activation='relu', name='dense', kernel_initializer=glorot_uniform(seed=0))(X)\n    X = Dropout(rate=0.5)(X)\n\n\n    # output stage\n    X_SBP = Dense(1, activation='relu', name='SBP', kernel_initializer=glorot_uniform(seed=0))(X)\n    X_DBP = Dense(1, activation='relu', name='DBP', kernel_initializer=glorot_uniform(seed=0))(X)\n    model = Model(inputs=X_input, outputs=[X_SBP, X_DBP], name='AlexNet_1D')\n    return model\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-17T20:16:25.794557Z","iopub.execute_input":"2022-08-17T20:16:25.795178Z","iopub.status.idle":"2022-08-17T20:16:25.813313Z","shell.execute_reply.started":"2022-08-17T20:16:25.795146Z","shell.execute_reply":"2022-08-17T20:16:25.81232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"CHECKPOINTS_FILE=\"/kaggle/input/outputstrainingphase1/logs/checkpoints/2022-17-08_alexnet_training_kaggle_cb.h5\"","metadata":{"execution":{"iopub.status.busy":"2022-08-18T21:21:40.822183Z","iopub.execute_input":"2022-08-18T21:21:40.82254Z","iopub.status.idle":"2022-08-18T21:21:40.827693Z","shell.execute_reply.started":"2022-08-18T21:21:40.82251Z","shell.execute_reply":"2022-08-18T21:21:40.826591Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\" train neural architectures using PPG data\n\nThis script trains a neural network using PPG data. The data is loaded from the the .tfrecord files created by the script\n'hdf_to_tfrecord.py'.\n\"\"\"\nimport csv\nimport os.path\nimport warnings\nfrom os.path import expanduser, join\nfrom os import environ\nfrom sys import argv\nfrom functools import partial\nfrom datetime import datetime\nimport argparse\n\nimport tensorflow as tf\n# tf.compat.v1.disable_eager_execution()\n\n\nimport pandas as pd\nimport numpy as np\nimport glob\n\nfrom tqdm import tqdm\n\n\n\ngpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device, True)\n\nBATCH_SIZE=54\nWIN_LEN=875\ndef read_tfrecord(example, win_len=WIN_LEN):\n    tfrecord_format = (\n        {\n            'ppg': tf.io.FixedLenFeature([win_len], tf.float32),\n            'label': tf.io.FixedLenFeature([2], tf.float32)\n        }\n    )\n    parsed_features = tf.io.parse_single_example(example, tfrecord_format)\n\n    return parsed_features['ppg'], (parsed_features['label'][0], parsed_features['label'][1])\n\n\ndef create_dataset(tfrecords_dir, tfrecord_basename, win_len=WIN_LEN, batch_size=BATCH_SIZE, modus='train'):\n    # pattern = join(tfrecords_dir, modus, tfrecord_basename + \"_\" + modus + \"_?????_of_?????.tfrecord\")\n    pattern = glob.glob(tfrecords_dir + f\"/{modus}/*.tfrecord\")\n    dataset = tf.data.TFRecordDataset.from_tensor_slices(pattern)\n\n    if modus == 'train':\n        dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n        dataset = dataset.interleave(\n            tf.data.TFRecordDataset,\n            cycle_length=800,\n            block_length=400)\n    else:\n        dataset = dataset.interleave(\n            tf.data.TFRecordDataset)\n\n    dataset = dataset.map(partial(read_tfrecord, win_len=win_len), num_parallel_calls=2)\n    dataset = dataset.shuffle(4096, reshuffle_each_iteration=True)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.repeat()\n\n    return dataset\n\n\ndef get_model(architecture, input_shape, UseDerivative=False):\n    return {\n        'alexnet': AlexNet_1D(input_shape, UseDerivative=UseDerivative),\n    }[architecture]\n\n\ndef ppg_train_mimic_iii(architecture,\n                        DataDir,\n                        ResultsDir,\n                        CheckpointDir,\n                        tensorboard_tag,\n                        tfrecord_basename,\n                        experiment_name,\n                        win_len=WIN_LEN,\n                        batch_size=BATCH_SIZE,\n                        lr=None,\n                        N_epochs=20,\n                        Ntrain=1e6,\n                        Nval=2.5e5,\n                        Ntest=2.5e5,\n                        UseDerivative=False,\n                        earlystopping=True):\n    # create datasets for training, validation and testing using .tfrecord files\n    test_dataset = create_dataset(DataDir, tfrecord_basename, win_len=win_len, batch_size=batch_size,\n                                  modus='test')\n    train_dataset = create_dataset(DataDir, tfrecord_basename, win_len=win_len, batch_size=batch_size, modus='train')\n    val_dataset = create_dataset(DataDir, tfrecord_basename, win_len=win_len, batch_size=batch_size,\n                                 modus='val')\n    data_in_shape = (win_len, 1)\n\n    # load the neurarchitecture\n    model = get_model(architecture, data_in_shape, UseDerivative=UseDerivative)\n\n    # callback for logging training and validation results\n    csvLogger_cb = tf.keras.callbacks.CSVLogger(\n        filename=join(ResultsDir, experiment_name + '_learningcurve.csv')\n    )\n\n    # checkpoint callback\n    cb_file=join(CheckpointDir, experiment_name + '_cb.h5')\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        filepath=cb_file,\n        save_best_only=True\n    )\n    \n    \n    \n    if os.path.exists(CHECKPOINTS_FILE):\n        model.load_weights(CHECKPOINTS_FILE)\n        print(\"Loaded weights from checkpoint\",CHECKPOINTS_FILE)\n\n    # tensorboard callback\n    tensorbard_cb = tf.keras.callbacks.TensorBoard(\n        log_dir=join(ResultsDir, 'tb', tensorboard_tag),\n        histogram_freq=0,\n        update_freq=\"batch\",\n        write_images=True,\n        write_graph=True    )\n\n    # callback for early stopping if validation loss stops improving\n    EarlyStopping_cb = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=10,\n        restore_best_weights=True\n    )\n\n\n    # define Adam optimizer\n    if lr is None:\n        opt = tf.keras.optimizers.Adam()\n    else:\n        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n\n    # compile model using mean squared error as loss function\n    model.compile(\n        optimizer=opt,\n        loss=tf.keras.losses.mean_squared_error,\n        metrics=[['mae'], ['mae']]\n    )\n\n#     class LRA(tf.keras.callbacks.Callback):\n#         def __init__(self, model, initial_learning_rate,frequency=1000,gamma=0.1):\n#             super(LRA, self).__init__()\n#             self.current_learning_rate=initial_learning_rate\n#             self.model=model\n#             self.frequency=frequency\n#             self.current_iter=0\n#             self.gamma=gamma\n\n#         def on_train_begin(self, logs=None):\n#             tf.keras.backend.set_value(self.model.optimizer.lr,\n#                                        self.current_learning_rate)\n\n#         def on_train_batch_end(self, batch, logs=None):\n#             self.current_iter+=1\n#             if self.current_iter%self.frequency==0:\n#                 self.current_learning_rate=self.current_learning_rate-self.current_learning_rate*self.gamma\n#                 tf.keras.backend.set_value(self.model.optimizer.lr, self.current_learning_rate)\n                \n#                 # print(\"Updating the learning rate to: \",self.current_learning_rate)\n\n    cb_list = [checkpoint_cb,\n               tensorbard_cb,\n               csvLogger_cb,\n               EarlyStopping_cb if earlystopping == True else [],\n               tf.keras.callbacks.ReduceLROnPlateau(monitor='mae', factor=0.2, patience=5)]\n\n\n\n\n\n    # # Perform Training and Validation\n    history = model.fit(\n        train_dataset,\n        steps_per_epoch=Ntrain // batch_size,\n        epochs=N_EPOCHS,\n        validation_data=val_dataset,\n        validation_steps=Nval // batch_size,\n        callbacks=cb_list\n    )\n    # Predictions on the testset\n    print(\"Loading weights from \", checkpoint_cb.filepath)\n    model.load_weights(checkpoint_cb.filepath)\n    test_results = pd.DataFrame({'SBP_true': [],\n                                 'DBP_true': [],\n                                 'SBP_est': [],\n                                 'DBP_est': []})\n\n    # store predictions on the test set as well as the corresponding ground truth in a csv file\n    test_dataset = iter(test_dataset)\n    for i in tqdm(range(int(Ntest // batch_size)), \"Running test\"):\n        ppg_test, BP_true = test_dataset.next()\n        BP_est = model.predict(ppg_test, verbose=0)\n        with warnings.catch_warnings():\n            TestBatchResult = pd.DataFrame({'SBP_true': BP_true[0].numpy(),\n                                            'DBP_true': BP_true[1].numpy(),\n                                            'SBP_est': np.squeeze(BP_est[0]),\n                                            'DBP_est': np.squeeze(BP_est[1]),\n                                            })\n            test_results = test_results.append(TestBatchResult)\n\n    ResultsFile = join(ResultsDir, experiment_name + '_test_results.csv')\n    test_results.to_csv(ResultsFile)\n\n    ResultsFileMae = join(ResultsDir, experiment_name + '_test_results_ae.csv')\n\n    sbp_mae = np.mean(np.abs(test_results[\"SBP_true\"] - test_results[\"SBP_est\"]))\n    sbp_aestd = np.std(np.abs(test_results[\"SBP_true\"] - test_results[\"SBP_est\"]))\n\n    dbp_mae = np.mean(np.abs(test_results[\"DBP_true\"] - test_results[\"DBP_est\"]))\n    dbp_aestd = np.std(np.abs(test_results[\"DBP_true\"] - test_results[\"DBP_est\"]))\n\n    with open(ResultsFileMae, \"w\") as output:\n        writer = csv.writer(output)\n        writer.writerow([\"sbp_mae\", \"sbp_aestd\", \"dbp_mae\", \"dbp_aestd\"])\n        writer.writerow([sbp_mae, sbp_aestd, dbp_mae, dbp_aestd])\n    test_results.to_csv(ResultsFile)\n\n    idx_min = np.argmin(history.history['val_loss'])\n\n    print(' Training finished')\n\n    return history.history['SBP_mae'][idx_min], history.history['DBP_mae'][idx_min], history.history['val_SBP_mae'][\n        idx_min], history.history['val_DBP_mae'][idx_min]\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-08-18T21:22:30.824257Z","iopub.execute_input":"2022-08-18T21:22:30.824704Z","iopub.status.idle":"2022-08-18T21:22:35.324741Z","shell.execute_reply.started":"2022-08-18T21:22:30.824667Z","shell.execute_reply":"2022-08-18T21:22:35.322141Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2022-08-18 21:22:35.198068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-18 21:22:35.316308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-18 21:22:35.317071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Launch training","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"architecture = \"alexnet\"\nexperiment_name = \"training_kaggle\"\nexperiment_name = datetime.now().strftime(\"%Y-%d-%m\") + '_' + architecture + '_' + experiment_name\n# experiment_name=\"2022-08-08_alexnet_training4\"\nROOT_DIR=\"/kaggle/working/logs\"\nDataDir = \"/kaggle/input/mmicdataset/train_test\"\nResultsDir = os.path.join(ROOT_DIR, \"results\")\nCheckpointDir = os.path.join(ROOT_DIR,\"checkpoints\")\ntb_tag = experiment_name\nlr = 0.001\nbatch_size = 128\nWIN_LEN = 875\nN_EPOCHS = 20\n\n\ntfrecord_basename = 'MIMIC_III_ppg'\n\nppg_train_mimic_iii(architecture,\n                    DataDir,\n                    ResultsDir,\n                    CheckpointDir,\n                    tb_tag,\n                    tfrecord_basename,\n                    experiment_name,\n                    win_len=WIN_LEN,\n                    batch_size=batch_size,\n                    lr=lr,\n                    N_epochs=N_EPOCHS,\n                    UseDerivative=False,\n                    earlystopping=False)  # False originally\n","metadata":{"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"collapsed":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}